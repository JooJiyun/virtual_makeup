{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"Code for training CycleGAN.\"\"\"\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from scipy.misc import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import data_loader, losses, model\n",
    "import cyclegan_datasets\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "#warnings.simplefilter('ignore', FutureWarning)\n",
    "#warnings.filterwarnings(action=\"ignore\",message=\".*regex.*\",category=DeprecationWarning)\n",
    "#warnings.filterwarnings(action=\"ignore\",message=\".*regex.*\",category=FutureWarning)\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "class CycleGAN:\n",
    "    \"\"\"The CycleGAN module.\"\"\"\n",
    "\n",
    "    def __init__(self, pool_size, lambda_a,\n",
    "                 lambda_b, output_root_dir, to_restore,\n",
    "                 base_lr, max_step,\n",
    "                 dataset_name, checkpoint_dir, do_flipping, skip):\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        self._pool_size = pool_size\n",
    "        self._size_before_crop = 286\n",
    "        self._lambda_a = lambda_a\n",
    "        self._lambda_b = lambda_b\n",
    "        self._output_dir = os.path.join(output_root_dir, current_time)\n",
    "        self._images_dir = os.path.join(self._output_dir, 'imgs')\n",
    "        self._num_imgs_to_save = 20\n",
    "        self._to_restore = to_restore\n",
    "        self._base_lr = base_lr\n",
    "        self._max_step = max_step\n",
    "        self._dataset_name = dataset_name\n",
    "        self._checkpoint_dir = checkpoint_dir\n",
    "        self._do_flipping = do_flipping\n",
    "        self._skip = skip\n",
    "\n",
    "        \n",
    "\n",
    "        self.fake_images_A = np.zeros(\n",
    "            (self._pool_size, 1, model.IMG_HEIGHT, model.IMG_WIDTH,\n",
    "             model.IMG_CHANNELS)\n",
    "        )\n",
    "        self.fake_images_B = np.zeros(\n",
    "            (self._pool_size, 1, model.IMG_HEIGHT, model.IMG_WIDTH,\n",
    "             model.IMG_CHANNELS)\n",
    "        )\n",
    "\n",
    "    def model_setup(self):\n",
    "        \"\"\"\n",
    "        This function sets up the model to train.\n",
    "\n",
    "        self.input_A/self.input_B -> Set of training images.\n",
    "        self.fake_A/self.fake_B -> Generated images by corresponding generator\n",
    "        of input_A and input_B\n",
    "        self.lr -> Learning rate variable\n",
    "        self.cyc_A/ self.cyc_B -> Images generated after feeding\n",
    "        self.fake_A/self.fake_B to corresponding generator.\n",
    "        This is use to calculate cyclic loss\n",
    "        \"\"\"\n",
    "        self.input_a = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                1,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"input_A\")\n",
    "        self.input_b = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                1,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"input_B\")\n",
    "        self.input_ref = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                1,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"input_ref\")\n",
    "\n",
    "        self.fake_pool_A = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                None,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"fake_pool_A\")\n",
    "        self.fake_pool_B = tf.placeholder(\n",
    "            tf.float32, [\n",
    "                None,\n",
    "                model.IMG_HEIGHT,\n",
    "                model.IMG_WIDTH,\n",
    "                model.IMG_CHANNELS\n",
    "            ], name=\"fake_pool_B\")\n",
    "\n",
    "        self.global_step = slim.get_or_create_global_step()\n",
    "\n",
    "        self.num_fake_inputs = 0\n",
    "\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
    "\n",
    "        \n",
    "        inputs = {\n",
    "            'images_a': self.input_a,\n",
    "            'images_b': self.input_b,\n",
    "            'images_ref': self.input_ref,\n",
    "            'fake_pool_a': self.fake_pool_A,\n",
    "            'fake_pool_b': self.fake_pool_B,\n",
    "        }\n",
    "\n",
    "        outputs = model.get_outputs(\n",
    "            inputs, skip=self._skip)\n",
    "\n",
    "        self.prob_real_a_is_real = outputs['prob_real_a_is_real']\n",
    "        self.prob_real_b_is_real = outputs['prob_real_b_is_real']\n",
    "        self.fake_images_a = outputs['fake_images_a']\n",
    "        self.fake_images_b = outputs['fake_images_b']\n",
    "        self.prob_fake_a_is_real = outputs['prob_fake_a_is_real']\n",
    "        self.prob_fake_b_is_real = outputs['prob_fake_b_is_real']\n",
    "\n",
    "        self.cycle_images_a = outputs['cycle_images_a']\n",
    "        self.cycle_images_b = outputs['cycle_images_b']\n",
    "\n",
    "        self.prob_fake_pool_a_is_real = outputs['prob_fake_pool_a_is_real']\n",
    "        self.prob_fake_pool_b_is_real = outputs['prob_fake_pool_b_is_real']\n",
    "\n",
    "    def compute_losses(self):\n",
    "        \"\"\"\n",
    "        In this function we are defining the variables for loss calculations\n",
    "        and training model.\n",
    "\n",
    "        d_loss_A/d_loss_B -> loss for discriminator A/B\n",
    "        g_loss_A/g_loss_B -> loss for generator A/B\n",
    "        *_trainer -> Various trainer for above loss functions\n",
    "        *_summ -> Summary variables for above loss functions\n",
    "        \"\"\"\n",
    "        cycle_consistency_loss_a = \\\n",
    "            self._lambda_a * losses.cycle_consistency_loss(\n",
    "                real_images=self.input_a, generated_images=self.cycle_images_a,\n",
    "            )\n",
    "        cycle_consistency_loss_b = \\\n",
    "            self._lambda_b * losses.cycle_consistency_loss(\n",
    "                real_images=self.input_b, generated_images=self.cycle_images_b,\n",
    "            )\n",
    "\n",
    "        lsgan_loss_a = losses.lsgan_loss_generator(self.prob_fake_a_is_real)\n",
    "        lsgan_loss_b = losses.lsgan_loss_generator(self.prob_fake_b_is_real)\n",
    "\n",
    "        g_loss_A = \\\n",
    "            cycle_consistency_loss_a + cycle_consistency_loss_b + lsgan_loss_b\n",
    "        g_loss_B = \\\n",
    "            cycle_consistency_loss_b + cycle_consistency_loss_a + lsgan_loss_a\n",
    "\n",
    "        d_loss_A = losses.lsgan_loss_discriminator(\n",
    "            prob_real_is_real=self.prob_real_a_is_real,\n",
    "            prob_fake_is_real=self.prob_fake_pool_a_is_real,\n",
    "        )\n",
    "        d_loss_B = losses.lsgan_loss_discriminator(\n",
    "            prob_real_is_real=self.prob_real_b_is_real,\n",
    "            prob_fake_is_real=self.prob_fake_pool_b_is_real,\n",
    "        )\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate, beta1=0.5)\n",
    "\n",
    "        self.model_vars = tf.trainable_variables()\n",
    "\n",
    "        d_A_vars = [var for var in self.model_vars if 'd_A' in var.name]\n",
    "        g_A_vars = [var for var in self.model_vars if 'g_A' in var.name]\n",
    "        d_B_vars = [var for var in self.model_vars if 'd_B' in var.name]\n",
    "        g_B_vars = [var for var in self.model_vars if 'g_B' in var.name]\n",
    "\n",
    "        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
    "        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
    "        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
    "        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n",
    "\n",
    "        for var in self.model_vars:\n",
    "            print(var.name)\n",
    "\n",
    "        # Summary variables for tensorboard\n",
    "        self.g_A_loss_summ = tf.summary.scalar(\"g_A_loss\", g_loss_A)\n",
    "        self.g_B_loss_summ = tf.summary.scalar(\"g_B_loss\", g_loss_B)\n",
    "        self.d_A_loss_summ = tf.summary.scalar(\"d_A_loss\", d_loss_A)\n",
    "        self.d_B_loss_summ = tf.summary.scalar(\"d_B_loss\", d_loss_B)\n",
    "\n",
    "    def save_images(self, sess, epoch):\n",
    "        \"\"\"\n",
    "        Saves input and output images.\n",
    "\n",
    "        :param sess: The session.\n",
    "        :param epoch: Currnt epoch.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self._images_dir):\n",
    "            os.makedirs(self._images_dir)\n",
    "\n",
    "        names = ['inputA_', 'inputB_', 'fakeA_',\n",
    "                 'fakeB_', 'cycA_', 'cycB_']\n",
    "\n",
    "        with open(os.path.join(\n",
    "                self._output_dir, 'epoch_' + str(epoch) + '.html'\n",
    "        ), 'w') as v_html:\n",
    "            for i in range(0, self._num_imgs_to_save):\n",
    "                print(\"Saving image {}/{}\".format(i, self._num_imgs_to_save))\n",
    "                inputs = sess.run(self.inputs)\n",
    "                filenames = sess.run(self.filenames)\n",
    "                fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([\n",
    "                    self.fake_images_a,\n",
    "                    self.fake_images_b,\n",
    "                    self.cycle_images_a,\n",
    "                    self.cycle_images_b\n",
    "                ], feed_dict={\n",
    "                    self.input_a: [inputs['images_i']],\n",
    "                    self.input_b: [inputs['images_j']],\n",
    "                    self.input_ref: [inputs['images_k']]\n",
    "                })\n",
    "\n",
    "                tensors = [inputs['images_i'], inputs['images_j'],\n",
    "                           fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp]\n",
    "                name_tensors = [str(filenames['filename_i'].decode()),\n",
    "                                str(filenames['filename_j'].decode()),\n",
    "                                str(filenames['filename_k'].decode())]\n",
    "                for filename in name_tensors:\n",
    "                    v_html.write(\n",
    "                         filename+\"  \"\n",
    "                    )\n",
    "                v_html.write(\"<br>\")\n",
    "                index = 0\n",
    "                for name, tensor in zip(names, tensors):\n",
    "                    image_name = name + str(epoch) + \"_\" + str(i)+ \".jpg\"\n",
    "                    imsave(os.path.join(self._images_dir, image_name),\n",
    "                           ((tensor[0] + 1) * 127.5).astype(np.uint8)\n",
    "                           )\n",
    "                    if index<2:\n",
    "                        image_name = filename[index]\n",
    "                    else:\n",
    "                        image_name = os.path.join('imgs', image_name)\n",
    "                    v_html.write(\n",
    "                        \"<img src=\\\"\" +image_name + \"\\\">\"\n",
    "                    )\n",
    "                    index+=1\n",
    "                    if index==2:v_html.write(\"<br>\")\n",
    "                v_html.write(\"<br>\")\n",
    "\n",
    "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
    "        \"\"\"\n",
    "        This function saves the generated image to corresponding\n",
    "        pool of images.\n",
    "\n",
    "        It keeps on feeling the pool till it is full and then randomly\n",
    "        selects an already stored image and replace it with new one.\n",
    "        \"\"\"\n",
    "        if num_fakes < self._pool_size:\n",
    "            fake_pool[num_fakes] = fake\n",
    "            return fake\n",
    "        else:\n",
    "            p = random.random()\n",
    "            if p > 0.5:\n",
    "                random_id = random.randint(0, self._pool_size - 1)\n",
    "                temp = fake_pool[random_id]\n",
    "                fake_pool[random_id] = fake\n",
    "                return temp\n",
    "            else:\n",
    "                return fake\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Training Function.\"\"\"\n",
    "        # Load Dataset from the dataset folder\n",
    "        self.inputs, self.filenames = data_loader.load_data(\n",
    "            self._dataset_name, self._size_before_crop,\n",
    "            self._do_flipping)\n",
    "\n",
    "        # Build the network\n",
    "        self.model_setup()\n",
    "\n",
    "        # Loss function calculations\n",
    "        self.compute_losses()\n",
    "\n",
    "        # Initializing the global variables\n",
    "        init = (tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        max_images = cyclegan_datasets.DATASET_TO_SIZES[self._dataset_name]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            # Restore the model to run the model from last checkpoint\n",
    "            if self._to_restore:\n",
    "                chkpt_fname = tf.train.latest_checkpoint(self._checkpoint_dir)\n",
    "                #chkpt_fname = self.restore_dir\n",
    "                saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            writer = tf.summary.FileWriter(self._output_dir)\n",
    "\n",
    "            if not os.path.exists(self._output_dir):\n",
    "                os.makedirs(self._output_dir)\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            # Training Loop\n",
    "            for epoch in range(sess.run(self.global_step), self._max_step):\n",
    "                print(\"In the epoch \", epoch)\n",
    "                saver.save(sess, os.path.join(\n",
    "                    self._output_dir, \"cyclegan\"), global_step=epoch)\n",
    "\n",
    "                # Dealing with the learning rate as per the epoch number\n",
    "                if epoch < 100:\n",
    "                    curr_lr = self._base_lr\n",
    "                else:\n",
    "                    curr_lr = self._base_lr - \\\n",
    "                        self._base_lr * (epoch - 100) / 100\n",
    "\n",
    "                self.save_images(sess, epoch)\n",
    "\n",
    "                for i in range(0, max_images):\n",
    "                    print(\"Processing batch {}/{}\".format(i, max_images))\n",
    "\n",
    "                    inputs = sess.run(self.inputs)\n",
    "                    filenams = sess.run(self.filenames)\n",
    "\n",
    "                    # Optimizing the G_A network\n",
    "                    _, fake_B_temp, summary_str = sess.run(\n",
    "                        [self.g_A_trainer,\n",
    "                         self.fake_images_b,\n",
    "                         self.g_A_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_a:\n",
    "                                [inputs['images_i']],\n",
    "                            self.input_b:\n",
    "                                [inputs['images_j']],\n",
    "                            self.input_ref:\n",
    "                                [inputs['images_k']],\n",
    "                            self.learning_rate: curr_lr\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    fake_B_temp1 = self.fake_image_pool(\n",
    "                        self.num_fake_inputs, fake_B_temp, self.fake_images_B)\n",
    "\n",
    "                    # Optimizing the D_B network\n",
    "                    _, summary_str = sess.run(\n",
    "                        [self.d_B_trainer, self.d_B_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_a:\n",
    "                                [inputs['images_i']],\n",
    "                            self.input_b:\n",
    "                                [inputs['images_j']],\n",
    "                            self.input_ref:\n",
    "                                [inputs['images_k']],\n",
    "                            self.learning_rate: curr_lr,\n",
    "                            self.fake_pool_B: fake_B_temp1\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    # Optimizing the G_B network\n",
    "                    _, fake_A_temp, summary_str = sess.run(\n",
    "                        [self.g_B_trainer,\n",
    "                         self.fake_images_a,\n",
    "                         self.g_B_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_a:\n",
    "                                [inputs['images_i']],\n",
    "                            self.input_b:\n",
    "                                [inputs['images_j']],\n",
    "                            self.input_ref:\n",
    "                                [inputs['images_k']],\n",
    "                            self.learning_rate: curr_lr\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    fake_A_temp1 = self.fake_image_pool(\n",
    "                        self.num_fake_inputs, fake_A_temp, self.fake_images_A)\n",
    "\n",
    "                    # Optimizing the D_A network\n",
    "                    _, summary_str = sess.run(\n",
    "                        [self.d_A_trainer, self.d_A_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_a:\n",
    "                                [inputs['images_i']],\n",
    "                            self.input_b:\n",
    "                                [inputs['images_j']],\n",
    "                            self.input_ref:\n",
    "                                [inputs['images_k']],\n",
    "                            self.learning_rate: curr_lr,\n",
    "                            self.fake_pool_A: fake_A_temp1\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    writer.flush()\n",
    "                    self.num_fake_inputs += 1\n",
    "\n",
    "                sess.run(tf.assign(self.global_step, epoch + 1))\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            writer.add_graph(sess.graph)\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test Function.\"\"\"\n",
    "        print(\"Testing the results\")\n",
    "\n",
    "        self.inputs,self.filenames = data_loader.load_data(\n",
    "            self._dataset_name, self._size_before_crop,\n",
    "            self._do_flipping)\n",
    "\n",
    "        self.model_setup()\n",
    "        saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            chkpt_fname = tf.train.latest_checkpoint(self._checkpoint_dir)\n",
    "\n",
    "            saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            self._num_imgs_to_save = cyclegan_datasets.DATASET_TO_SIZES[\n",
    "                self._dataset_name]\n",
    "            self.save_images(sess, 0)\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "\n",
    "def main(to_train, log_dir, checkpoint_dir):\n",
    "    \"\"\"\n",
    "\n",
    "    :param to_train: Specify whether it is training or testing. 1: training; 2:\n",
    "     resuming from latest checkpoint; 0: testing.\n",
    "    :param log_dir: The root dir to save checkpoints and imgs. The actual dir\n",
    "    is the root dir appended by the folder with the name timestamp.\n",
    "    :param config_filename: The configuration file.\n",
    "    :param checkpoint_dir: The directory that saves the latest checkpoint. It\n",
    "    only takes effect when to_train == 2.\n",
    "    :param skip: A boolean indicating whether to add skip connection between\n",
    "    input and output.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    lambda_a = 10.0\n",
    "    lambda_b = 10.0\n",
    "    pool_size = 50\n",
    "\n",
    "    to_restore = (to_train == 2)\n",
    "    base_lr = 0.0002\n",
    "    max_step = 200\n",
    "    dataset_name = 'lipstick_data'\n",
    "    do_flipping = False\n",
    "    skip = False\n",
    "\n",
    "\n",
    "    cyclegan_model = CycleGAN(pool_size, lambda_a, lambda_b, log_dir,\n",
    "                              to_restore, base_lr, max_step,\n",
    "                              dataset_name, checkpoint_dir, do_flipping, skip)\n",
    "\n",
    "    \n",
    "    if to_train > 0:\n",
    "        cyclegan_model.train()\n",
    "    else:\n",
    "        cyclegan_model.test()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the results\n",
      "WARNING:tensorflow:From /home/joo/Desktop/cycleGAN/py/data_loader.py:16: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/joo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/training/input.py:276: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/joo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/joo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/joo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/joo/Desktop/cycleGAN/py/data_loader.py:18: TextLineReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TextLineDataset`.\n",
      "WARNING:tensorflow:From /home/joo/Desktop/cycleGAN/py/data_loader.py:94: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-1-df4112a4580f>:112: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "INFO:tensorflow:Restoring parameters from ./log/20190219-163649/cyclegan-199\n",
      "WARNING:tensorflow:From <ipython-input-1-df4112a4580f>:439: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "Saving image 0/630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joo/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/ipykernel_launcher.py:247: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image 1/630\n",
      "Saving image 2/630\n",
      "Saving image 3/630\n",
      "Saving image 4/630\n",
      "Saving image 5/630\n",
      "Saving image 6/630\n",
      "Saving image 7/630\n",
      "Saving image 8/630\n",
      "Saving image 9/630\n",
      "Saving image 10/630\n",
      "Saving image 11/630\n",
      "Saving image 12/630\n",
      "Saving image 13/630\n",
      "Saving image 14/630\n",
      "Saving image 15/630\n",
      "Saving image 16/630\n",
      "Saving image 17/630\n",
      "Saving image 18/630\n",
      "Saving image 19/630\n",
      "Saving image 20/630\n",
      "Saving image 21/630\n",
      "Saving image 22/630\n",
      "Saving image 23/630\n",
      "Saving image 24/630\n",
      "Saving image 25/630\n",
      "Saving image 26/630\n",
      "Saving image 27/630\n",
      "Saving image 28/630\n",
      "Saving image 29/630\n",
      "Saving image 30/630\n",
      "Saving image 31/630\n",
      "Saving image 32/630\n",
      "Saving image 33/630\n",
      "Saving image 34/630\n",
      "Saving image 35/630\n",
      "Saving image 36/630\n",
      "Saving image 37/630\n",
      "Saving image 38/630\n",
      "Saving image 39/630\n",
      "Saving image 40/630\n",
      "Saving image 41/630\n",
      "Saving image 42/630\n",
      "Saving image 43/630\n",
      "Saving image 44/630\n",
      "Saving image 45/630\n",
      "Saving image 46/630\n",
      "Saving image 47/630\n",
      "Saving image 48/630\n",
      "Saving image 49/630\n",
      "Saving image 50/630\n",
      "Saving image 51/630\n",
      "Saving image 52/630\n",
      "Saving image 53/630\n",
      "Saving image 54/630\n",
      "Saving image 55/630\n",
      "Saving image 56/630\n",
      "Saving image 57/630\n",
      "Saving image 58/630\n",
      "Saving image 59/630\n",
      "Saving image 60/630\n",
      "Saving image 61/630\n",
      "Saving image 62/630\n",
      "Saving image 63/630\n",
      "Saving image 64/630\n",
      "Saving image 65/630\n",
      "Saving image 66/630\n",
      "Saving image 67/630\n",
      "Saving image 68/630\n",
      "Saving image 69/630\n",
      "Saving image 70/630\n",
      "Saving image 71/630\n",
      "Saving image 72/630\n",
      "Saving image 73/630\n",
      "Saving image 74/630\n",
      "Saving image 75/630\n",
      "Saving image 76/630\n",
      "Saving image 77/630\n",
      "Saving image 78/630\n",
      "Saving image 79/630\n",
      "Saving image 80/630\n",
      "Saving image 81/630\n",
      "Saving image 82/630\n",
      "Saving image 83/630\n",
      "Saving image 84/630\n",
      "Saving image 85/630\n",
      "Saving image 86/630\n",
      "Saving image 87/630\n",
      "Saving image 88/630\n",
      "Saving image 89/630\n",
      "Saving image 90/630\n",
      "Saving image 91/630\n",
      "Saving image 92/630\n",
      "Saving image 93/630\n",
      "Saving image 94/630\n",
      "Saving image 95/630\n",
      "Saving image 96/630\n",
      "Saving image 97/630\n",
      "Saving image 98/630\n",
      "Saving image 99/630\n",
      "Saving image 100/630\n",
      "Saving image 101/630\n",
      "Saving image 102/630\n",
      "Saving image 103/630\n",
      "Saving image 104/630\n",
      "Saving image 105/630\n",
      "Saving image 106/630\n",
      "Saving image 107/630\n",
      "Saving image 108/630\n",
      "Saving image 109/630\n",
      "Saving image 110/630\n",
      "Saving image 111/630\n",
      "Saving image 112/630\n",
      "Saving image 113/630\n",
      "Saving image 114/630\n",
      "Saving image 115/630\n",
      "Saving image 116/630\n",
      "Saving image 117/630\n",
      "Saving image 118/630\n",
      "Saving image 119/630\n",
      "Saving image 120/630\n",
      "Saving image 121/630\n",
      "Saving image 122/630\n",
      "Saving image 123/630\n",
      "Saving image 124/630\n",
      "Saving image 125/630\n",
      "Saving image 126/630\n",
      "Saving image 127/630\n",
      "Saving image 128/630\n",
      "Saving image 129/630\n",
      "Saving image 130/630\n",
      "Saving image 131/630\n",
      "Saving image 132/630\n",
      "Saving image 133/630\n",
      "Saving image 134/630\n",
      "Saving image 135/630\n",
      "Saving image 136/630\n",
      "Saving image 137/630\n",
      "Saving image 138/630\n",
      "Saving image 139/630\n",
      "Saving image 140/630\n",
      "Saving image 141/630\n",
      "Saving image 142/630\n",
      "Saving image 143/630\n",
      "Saving image 144/630\n",
      "Saving image 145/630\n",
      "Saving image 146/630\n",
      "Saving image 147/630\n",
      "Saving image 148/630\n",
      "Saving image 149/630\n",
      "Saving image 150/630\n",
      "Saving image 151/630\n",
      "Saving image 152/630\n",
      "Saving image 153/630\n",
      "Saving image 154/630\n",
      "Saving image 155/630\n",
      "Saving image 156/630\n",
      "Saving image 157/630\n",
      "Saving image 158/630\n",
      "Saving image 159/630\n",
      "Saving image 160/630\n",
      "Saving image 161/630\n",
      "Saving image 162/630\n",
      "Saving image 163/630\n",
      "Saving image 164/630\n",
      "Saving image 165/630\n",
      "Saving image 166/630\n",
      "Saving image 167/630\n",
      "Saving image 168/630\n",
      "Saving image 169/630\n",
      "Saving image 170/630\n",
      "Saving image 171/630\n",
      "Saving image 172/630\n",
      "Saving image 173/630\n",
      "Saving image 174/630\n",
      "Saving image 175/630\n",
      "Saving image 176/630\n",
      "Saving image 177/630\n",
      "Saving image 178/630\n",
      "Saving image 179/630\n",
      "Saving image 180/630\n",
      "Saving image 181/630\n",
      "Saving image 182/630\n",
      "Saving image 183/630\n",
      "Saving image 184/630\n",
      "Saving image 185/630\n",
      "Saving image 186/630\n",
      "Saving image 187/630\n",
      "Saving image 188/630\n",
      "Saving image 189/630\n",
      "Saving image 190/630\n",
      "Saving image 191/630\n",
      "Saving image 192/630\n",
      "Saving image 193/630\n",
      "Saving image 194/630\n",
      "Saving image 195/630\n",
      "Saving image 196/630\n",
      "Saving image 197/630\n",
      "Saving image 198/630\n",
      "Saving image 199/630\n",
      "Saving image 200/630\n",
      "Saving image 201/630\n",
      "Saving image 202/630\n",
      "Saving image 203/630\n",
      "Saving image 204/630\n",
      "Saving image 205/630\n",
      "Saving image 206/630\n",
      "Saving image 207/630\n",
      "Saving image 208/630\n",
      "Saving image 209/630\n",
      "Saving image 210/630\n",
      "Saving image 211/630\n",
      "Saving image 212/630\n",
      "Saving image 213/630\n",
      "Saving image 214/630\n",
      "Saving image 215/630\n",
      "Saving image 216/630\n",
      "Saving image 217/630\n",
      "Saving image 218/630\n",
      "Saving image 219/630\n",
      "Saving image 220/630\n",
      "Saving image 221/630\n",
      "Saving image 222/630\n",
      "Saving image 223/630\n",
      "Saving image 224/630\n",
      "Saving image 225/630\n",
      "Saving image 226/630\n",
      "Saving image 227/630\n",
      "Saving image 228/630\n",
      "Saving image 229/630\n",
      "Saving image 230/630\n",
      "Saving image 231/630\n",
      "Saving image 232/630\n",
      "Saving image 233/630\n",
      "Saving image 234/630\n",
      "Saving image 235/630\n",
      "Saving image 236/630\n",
      "Saving image 237/630\n",
      "Saving image 238/630\n",
      "Saving image 239/630\n",
      "Saving image 240/630\n",
      "Saving image 241/630\n",
      "Saving image 242/630\n",
      "Saving image 243/630\n",
      "Saving image 244/630\n",
      "Saving image 245/630\n",
      "Saving image 246/630\n",
      "Saving image 247/630\n",
      "Saving image 248/630\n",
      "Saving image 249/630\n",
      "Saving image 250/630\n",
      "Saving image 251/630\n",
      "Saving image 252/630\n",
      "Saving image 253/630\n",
      "Saving image 254/630\n",
      "Saving image 255/630\n",
      "Saving image 256/630\n",
      "Saving image 257/630\n",
      "Saving image 258/630\n",
      "Saving image 259/630\n",
      "Saving image 260/630\n",
      "Saving image 261/630\n",
      "Saving image 262/630\n",
      "Saving image 263/630\n",
      "Saving image 264/630\n",
      "Saving image 265/630\n",
      "Saving image 266/630\n",
      "Saving image 267/630\n",
      "Saving image 268/630\n",
      "Saving image 269/630\n",
      "Saving image 270/630\n",
      "Saving image 271/630\n",
      "Saving image 272/630\n",
      "Saving image 273/630\n",
      "Saving image 274/630\n",
      "Saving image 275/630\n",
      "Saving image 276/630\n",
      "Saving image 277/630\n",
      "Saving image 278/630\n",
      "Saving image 279/630\n",
      "Saving image 280/630\n",
      "Saving image 281/630\n",
      "Saving image 282/630\n",
      "Saving image 283/630\n",
      "Saving image 284/630\n",
      "Saving image 285/630\n",
      "Saving image 286/630\n",
      "Saving image 287/630\n",
      "Saving image 288/630\n",
      "Saving image 289/630\n",
      "Saving image 290/630\n",
      "Saving image 291/630\n",
      "Saving image 292/630\n",
      "Saving image 293/630\n",
      "Saving image 294/630\n",
      "Saving image 295/630\n",
      "Saving image 296/630\n",
      "Saving image 297/630\n",
      "Saving image 298/630\n",
      "Saving image 299/630\n",
      "Saving image 300/630\n",
      "Saving image 301/630\n",
      "Saving image 302/630\n",
      "Saving image 303/630\n",
      "Saving image 304/630\n",
      "Saving image 305/630\n",
      "Saving image 306/630\n",
      "Saving image 307/630\n",
      "Saving image 308/630\n",
      "Saving image 309/630\n",
      "Saving image 310/630\n",
      "Saving image 311/630\n",
      "Saving image 312/630\n",
      "Saving image 313/630\n",
      "Saving image 314/630\n",
      "Saving image 315/630\n",
      "Saving image 316/630\n",
      "Saving image 317/630\n",
      "Saving image 318/630\n",
      "Saving image 319/630\n",
      "Saving image 320/630\n",
      "Saving image 321/630\n",
      "Saving image 322/630\n",
      "Saving image 323/630\n",
      "Saving image 324/630\n",
      "Saving image 325/630\n",
      "Saving image 326/630\n",
      "Saving image 327/630\n",
      "Saving image 328/630\n",
      "Saving image 329/630\n",
      "Saving image 330/630\n",
      "Saving image 331/630\n",
      "Saving image 332/630\n",
      "Saving image 333/630\n",
      "Saving image 334/630\n",
      "Saving image 335/630\n",
      "Saving image 336/630\n",
      "Saving image 337/630\n",
      "Saving image 338/630\n",
      "Saving image 339/630\n",
      "Saving image 340/630\n",
      "Saving image 341/630\n",
      "Saving image 342/630\n",
      "Saving image 343/630\n",
      "Saving image 344/630\n",
      "Saving image 345/630\n",
      "Saving image 346/630\n",
      "Saving image 347/630\n",
      "Saving image 348/630\n",
      "Saving image 349/630\n",
      "Saving image 350/630\n",
      "Saving image 351/630\n",
      "Saving image 352/630\n",
      "Saving image 353/630\n",
      "Saving image 354/630\n",
      "Saving image 355/630\n",
      "Saving image 356/630\n",
      "Saving image 357/630\n",
      "Saving image 358/630\n",
      "Saving image 359/630\n",
      "Saving image 360/630\n",
      "Saving image 361/630\n",
      "Saving image 362/630\n",
      "Saving image 363/630\n",
      "Saving image 364/630\n",
      "Saving image 365/630\n",
      "Saving image 366/630\n",
      "Saving image 367/630\n",
      "Saving image 368/630\n",
      "Saving image 369/630\n",
      "Saving image 370/630\n",
      "Saving image 371/630\n",
      "Saving image 372/630\n",
      "Saving image 373/630\n",
      "Saving image 374/630\n",
      "Saving image 375/630\n",
      "Saving image 376/630\n",
      "Saving image 377/630\n",
      "Saving image 378/630\n",
      "Saving image 379/630\n",
      "Saving image 380/630\n",
      "Saving image 381/630\n",
      "Saving image 382/630\n",
      "Saving image 383/630\n",
      "Saving image 384/630\n",
      "Saving image 385/630\n",
      "Saving image 386/630\n",
      "Saving image 387/630\n",
      "Saving image 388/630\n",
      "Saving image 389/630\n",
      "Saving image 390/630\n",
      "Saving image 391/630\n",
      "Saving image 392/630\n",
      "Saving image 393/630\n",
      "Saving image 394/630\n",
      "Saving image 395/630\n",
      "Saving image 396/630\n",
      "Saving image 397/630\n",
      "Saving image 398/630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving image 399/630\n",
      "Saving image 400/630\n",
      "Saving image 401/630\n",
      "Saving image 402/630\n",
      "Saving image 403/630\n",
      "Saving image 404/630\n",
      "Saving image 405/630\n",
      "Saving image 406/630\n",
      "Saving image 407/630\n",
      "Saving image 408/630\n",
      "Saving image 409/630\n",
      "Saving image 410/630\n",
      "Saving image 411/630\n",
      "Saving image 412/630\n",
      "Saving image 413/630\n",
      "Saving image 414/630\n",
      "Saving image 415/630\n",
      "Saving image 416/630\n",
      "Saving image 417/630\n",
      "Saving image 418/630\n",
      "Saving image 419/630\n",
      "Saving image 420/630\n",
      "Saving image 421/630\n",
      "Saving image 422/630\n",
      "Saving image 423/630\n",
      "Saving image 424/630\n",
      "Saving image 425/630\n",
      "Saving image 426/630\n",
      "Saving image 427/630\n",
      "Saving image 428/630\n",
      "Saving image 429/630\n",
      "Saving image 430/630\n",
      "Saving image 431/630\n",
      "Saving image 432/630\n",
      "Saving image 433/630\n",
      "Saving image 434/630\n",
      "Saving image 435/630\n",
      "Saving image 436/630\n",
      "Saving image 437/630\n",
      "Saving image 438/630\n",
      "Saving image 439/630\n",
      "Saving image 440/630\n",
      "Saving image 441/630\n",
      "Saving image 442/630\n",
      "Saving image 443/630\n",
      "Saving image 444/630\n",
      "Saving image 445/630\n",
      "Saving image 446/630\n",
      "Saving image 447/630\n",
      "Saving image 448/630\n",
      "Saving image 449/630\n",
      "Saving image 450/630\n",
      "Saving image 451/630\n",
      "Saving image 452/630\n",
      "Saving image 453/630\n",
      "Saving image 454/630\n",
      "Saving image 455/630\n",
      "Saving image 456/630\n",
      "Saving image 457/630\n",
      "Saving image 458/630\n",
      "Saving image 459/630\n",
      "Saving image 460/630\n",
      "Saving image 461/630\n",
      "Saving image 462/630\n",
      "Saving image 463/630\n",
      "Saving image 464/630\n",
      "Saving image 465/630\n",
      "Saving image 466/630\n",
      "Saving image 467/630\n",
      "Saving image 468/630\n",
      "Saving image 469/630\n",
      "Saving image 470/630\n",
      "Saving image 471/630\n",
      "Saving image 472/630\n",
      "Saving image 473/630\n",
      "Saving image 474/630\n",
      "Saving image 475/630\n",
      "Saving image 476/630\n",
      "Saving image 477/630\n",
      "Saving image 478/630\n",
      "Saving image 479/630\n",
      "Saving image 480/630\n",
      "Saving image 481/630\n",
      "Saving image 482/630\n",
      "Saving image 483/630\n",
      "Saving image 484/630\n",
      "Saving image 485/630\n",
      "Saving image 486/630\n",
      "Saving image 487/630\n",
      "Saving image 488/630\n",
      "Saving image 489/630\n",
      "Saving image 490/630\n",
      "Saving image 491/630\n",
      "Saving image 492/630\n",
      "Saving image 493/630\n",
      "Saving image 494/630\n",
      "Saving image 495/630\n",
      "Saving image 496/630\n",
      "Saving image 497/630\n",
      "Saving image 498/630\n",
      "Saving image 499/630\n",
      "Saving image 500/630\n",
      "Saving image 501/630\n",
      "Saving image 502/630\n",
      "Saving image 503/630\n",
      "Saving image 504/630\n",
      "Saving image 505/630\n",
      "Saving image 506/630\n",
      "Saving image 507/630\n",
      "Saving image 508/630\n",
      "Saving image 509/630\n",
      "Saving image 510/630\n",
      "Saving image 511/630\n",
      "Saving image 512/630\n",
      "Saving image 513/630\n",
      "Saving image 514/630\n",
      "Saving image 515/630\n",
      "Saving image 516/630\n",
      "Saving image 517/630\n",
      "Saving image 518/630\n",
      "Saving image 519/630\n",
      "Saving image 520/630\n",
      "Saving image 521/630\n",
      "Saving image 522/630\n",
      "Saving image 523/630\n",
      "Saving image 524/630\n",
      "Saving image 525/630\n",
      "Saving image 526/630\n",
      "Saving image 527/630\n",
      "Saving image 528/630\n",
      "Saving image 529/630\n",
      "Saving image 530/630\n",
      "Saving image 531/630\n",
      "Saving image 532/630\n",
      "Saving image 533/630\n",
      "Saving image 534/630\n",
      "Saving image 535/630\n",
      "Saving image 536/630\n",
      "Saving image 537/630\n",
      "Saving image 538/630\n",
      "Saving image 539/630\n",
      "Saving image 540/630\n",
      "Saving image 541/630\n",
      "Saving image 542/630\n",
      "Saving image 543/630\n",
      "Saving image 544/630\n",
      "Saving image 545/630\n",
      "Saving image 546/630\n",
      "Saving image 547/630\n",
      "Saving image 548/630\n",
      "Saving image 549/630\n",
      "Saving image 550/630\n",
      "Saving image 551/630\n",
      "Saving image 552/630\n",
      "Saving image 553/630\n",
      "Saving image 554/630\n",
      "Saving image 555/630\n",
      "Saving image 556/630\n",
      "Saving image 557/630\n",
      "Saving image 558/630\n",
      "Saving image 559/630\n",
      "Saving image 560/630\n",
      "Saving image 561/630\n",
      "Saving image 562/630\n",
      "Saving image 563/630\n",
      "Saving image 564/630\n",
      "Saving image 565/630\n",
      "Saving image 566/630\n",
      "Saving image 567/630\n",
      "Saving image 568/630\n",
      "Saving image 569/630\n",
      "Saving image 570/630\n",
      "Saving image 571/630\n",
      "Saving image 572/630\n",
      "Saving image 573/630\n",
      "Saving image 574/630\n",
      "Saving image 575/630\n",
      "Saving image 576/630\n",
      "Saving image 577/630\n",
      "Saving image 578/630\n",
      "Saving image 579/630\n",
      "Saving image 580/630\n",
      "Saving image 581/630\n",
      "Saving image 582/630\n",
      "Saving image 583/630\n",
      "Saving image 584/630\n",
      "Saving image 585/630\n",
      "Saving image 586/630\n",
      "Saving image 587/630\n",
      "Saving image 588/630\n",
      "Saving image 589/630\n",
      "Saving image 590/630\n",
      "Saving image 591/630\n",
      "Saving image 592/630\n",
      "Saving image 593/630\n",
      "Saving image 594/630\n",
      "Saving image 595/630\n",
      "Saving image 596/630\n",
      "Saving image 597/630\n",
      "Saving image 598/630\n",
      "Saving image 599/630\n",
      "Saving image 600/630\n",
      "Saving image 601/630\n",
      "Saving image 602/630\n",
      "Saving image 603/630\n",
      "Saving image 604/630\n",
      "Saving image 605/630\n",
      "Saving image 606/630\n",
      "Saving image 607/630\n",
      "Saving image 608/630\n",
      "Saving image 609/630\n",
      "Saving image 610/630\n",
      "Saving image 611/630\n",
      "Saving image 612/630\n",
      "Saving image 613/630\n",
      "Saving image 614/630\n",
      "Saving image 615/630\n",
      "Saving image 616/630\n",
      "Saving image 617/630\n",
      "Saving image 618/630\n",
      "Saving image 619/630\n",
      "Saving image 620/630\n",
      "Saving image 621/630\n",
      "Saving image 622/630\n",
      "Saving image 623/630\n",
      "Saving image 624/630\n",
      "Saving image 625/630\n",
      "Saving image 626/630\n",
      "Saving image 627/630\n",
      "Saving image 628/630\n",
      "Saving image 629/630\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    log_dir = \"./log\"\n",
    "    main(0, log_dir,\"./log/20190219-163649\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
